{
  "hash": "5a358bf22a8f03a255a07d742f8aae5e",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Tuning Random Planted Forests with the help of a Random Planted Forest\"\ndate: \"2024-01-28\"\nabstract: |\n  In which we use an untuned model to explain the tuning of a tuned model just to see how it goes.\ncategories: \n  - ml\n  - \"xai/iml\"\ntags: \n  - random planted forest\n  - glex\n  - functional decomposition\n  - hpo\nimage: \"vi.png\"\neditor_options: \n  chunk_output_type: console\n---\n\n\n\n\nWhen we talk about machine learning interpretability methods, we tend to circle back to similar data examples.\nPartially because there's a benefit to the familiarity and partially because, well, there's just a limited number of real-world datasets floating around out there which are both publicly accessible and exhibit some kind of interesting structure that justifies the investigation of, say, third-order interaction effects with some sort of intuitive interpretation.\n\nFor IML, the `Bikeshare` data is one of those popular datasets.\nWe're using it for a showcase article of the [`glex`][glexgithub] R package, and this post is decidedly not about that --- but feel free to [read the paper][glexpaper].\n\nWhat this post is actually about is [Random Planted Forests][rpfarxiv] (RPF, [R package on GitHub][rpfgithub]).\nI wanted the usage example on the `Bikeshare` data to be interesting and useful, and since interpretability methods tend to only be as good as the models they're trying to explain, I first needed a decent model.\n\nSo, that's what this post is about: Tuning RPF, and then using RPF to explain the tuning results.\n\n## The Data\n\nWe're using the [`Bikeshare` data as included with the `ISLR2` package](https://search.r-project.org/CRAN/refmans/ISLR2/html/Bikeshare.html) (originally from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/dataset/275/bike+sharing+dataset)), which I preprocessed and whittled down a little for simplicity's sake.\nYou can look at the preprocessing steps in the code below, but I'll skip the dataset exploration as it's not the focus of this post.\n\n<details>\n<summary>Show preprocessing code</summary>\n\n```r\nlibrary(data.table)\nif (!(\"ISLR2\" %in% installed.packages())) {\n  install.packages(\"ISLR2\")\n}\n\ndata(\"Bikeshare\", package = \"ISLR2\")\n\nbike <- data.table(Bikeshare)\nbike[, hr := as.numeric(as.character(hr))]\nbike[, workingday := factor(workingday, levels = c(0, 1), labels = c(\"No Workingday\", \"Workingday\"))]\nbike[, season := factor(season, levels = 1:4, labels = c(\"Winter\", \"Spring\", \"Summer\", \"Fall\"))]\nbike[, atemp := NULL]\nbike[, day := NULL]\nbike[, registered := NULL]\nbike[, casual := NULL]\n\nsaveRDS(bike, \"bike.rds\")\n```\n\n</details>\n\n\n## Tuning with `mlr3`\n\nI wrapped the RPF learner into an `mlr3` learner for `mlr3extralearners`, which makes it very convenient to tune.\nIf you're unfamiliar with the `mlr3` ecosystem, well guess who contributed to the now-published mlr3 book [available for free online](https://mlr3book.mlr-org.com/) or buy from your [friendly neighborhood dystopian online retailer in fancy tree corpse form](https://www.amazon.com/dp/1032507543).  \nNo judgement.\n\nAnyway, here's the code I used, which is fairly standard \"wrap learner in `AutoTuner` and tune the thing with 3-fold cross-validation and a somewhat arbitrary tuning budget using MBO I guess because why not\" (WLATT3FCVSATBUMBOIGBWN, as my grampa used to call it).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(mlr3verse)\nlibrary(mlr3extralearners)\n# install.packages(\"mlr3extralearners\", repos = \"https://mlr-org.r-universe.dev\")\n\nbike <- readRDS(\"bike.rds\")\nbiketask <- as_task_regr(bike, target = \"bikers\")\nsplits <- partition(biketask)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntuned_rpf <- auto_tuner(\n  learner = lrn(\"regr.rpf\", ntrees = 200, max_interaction = 4, nthreads = 2),\n  tuner = tnr(\"mbo\"),\n  resampling = rsmp(\"cv\", folds = 3),\n  terminator = trm(\"evals\", n_evals = 100, k = 10),\n  search_space = ps(\n    max_interaction = p_int(2, 10),\n    splits = p_int(10, 100),\n    split_try = p_int(1, 20),\n    t_try = p_dbl(0.1, 1)\n  ),\n  store_tuning_instance = TRUE, \n  store_benchmark_result = TRUE\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntuned_rpf$train(biketask, row_ids = splits$train)\n```\n:::\n\n\nIdeally, we would evaluate the tuned RPF on the test dataset like this:\n\n```r\npred <- tuned_rpf$predict(biketask, row_ids = splits$test)\npred$score(msr(\"regr.rmsle\"))\n```\n\n...but since I saved the learner with `saveRDS()` on a different machine and restored it here for use with this post, we only get the error message\n\n```\nError:\n! external pointer is not valid\n```\n\nThis is related to RPF using [Rcpp modules](https://cran.r-project.org/web/packages/Rcpp/vignettes/Rcpp-modules.pdf) under the hood, with the takeaway being that at the time of writing I don't know if there's a way to serialize and deserialize RPF models for situations like this.\nThis is quite unfortunate, but for the time being we'll just have to assume that the tuning result is somewhat reasonable.\nI should have just tuned on the full datasets, but alas, I guess this will have to do now.\n\nAnyways, we extract the tuning archive stored in `$archive` of the `AutoTuner` object and convert the MSE we initially tuned with to the RMSE just to get a more manageable range of scores.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(data.table)\narchive <- tuned_rpf$archive$data\narchive[, regr.rmse := sqrt(regr.mse)]\n```\n:::\n\n\nThe [kaggle challenge](https://www.kaggle.com/c/bike-sharing-demand) for this dataset (or a version of it, anyway) evaluates using the RMSLE, which would probably be more appropriate, come to think of it.\nPutting that one on the \"oh well, next time\" pile.\n\nLet's take a look at our scores in relation to our hyperparameter configurations first --- one at a time, ignoring any interdependencies.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nmelt(archive, id.vars = \"regr.rmse\", \n     measure.vars = c(\"splits\", \"split_try\", \"t_try\", \"max_interaction\")) |>\n  ggplot(aes(x = value, y = regr.rmse)) +\n  facet_wrap(vars(variable), scales = \"free_x\") +\n  geom_point() +\n  labs(\n    title = \"RPF / Bikeshare tuning archive\",\n    subtitle = \"Scores on inner resampling folds\",\n    x = \"Parameter Value\", y = \"RMSE\"\n  )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/archive-plots-1.png){width=672}\n:::\n:::\n\n\nThe main thing to note here is that \"more `splits` more good\", while the picture for the other parameters isn't as clear.\nOther parameters might interact, and overall it's not obvious if a parameter is more important than another.\n\nWould be nice if we could somehow functionally decompose the effects of these parameters up to arbitrary ord--- oh wait that's `glex`, yes, let's do the `glex` thing.\n\n## Explaining the Tuning Results\n\nBoth `glex` and `randomPlantedForest` can be installed via [r-universe](https://r-universe.dev/) if you can't be bothered to type `remotes::install_github(\"PlantedML/glex\")` and/or `remotes::install_github(\"PlantedML/randomPlantedForest\")`, which I usually can't:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall.packages(c(\"glex\", \"randomPlantedForest\"), repos = \"https://plantedml.r-universe.dev\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(randomPlantedForest)\nlibrary(glex)\n```\n:::\n\n\nWe then fit another RPF with heuristically picked parameters on the tuning archive, using the RMSE as target and tuning parameters as features.\nWhy not tune RPF \"properly\" here, you ask?\nBecause I can't decide whether I want to make this post a recursion joke or not.\nAlso time is finite and I couldn't be bothered.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrpfit <- rpf(\n  regr.rmse ~ splits + split_try + t_try + max_interaction, \n  data = archive, \n  ntrees = 100, \n  splits = 100, \n  split_try = 20, \n  t_try = 1, \n  max_interaction = 4,\n  nthreads = 3\n)\n\nrpglex <- glex(rpfit, tuned_rpf$archive$data)\n```\n:::\n\n\n\n\n\n\n### Variable Importance\n\nLet's take a first look at the variable importance scores, calculated as the mean absolute contribution to RMSE of each main- or interaction effect, respectively.\nThe nice thing about this is that we can quantify the relevance of each tuning parameter while fully taking into account any interaction with other parameters, _and also_ quantify the overall relevance of e.g. second-order interactions compared to main effects only.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrpvi <- glex_vi(rpglex)\n\nautoplot(rpvi)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/vi-1.png){width=672}\n:::\n\n```{.r .cell-code}\nautoplot(rpvi, by_degree = TRUE)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/vi-2.png){width=672}\n:::\n:::\n\n\n...which definitely could be interesting in some case that is apparently not this one!\nSo yeah.\n\nTurns out `splits` has by far the largest effect, then we see `t_try` and `max_interaction` far behind, while `split_try` actually turns out to be less influential than its interaction effects with `t_try` and `max_interaction`?\nOkay?\nSure, why not.\nI guess it's a good thing to see confirmation that, interactions of the 3rd or 4th degree are negligible, and the second-order interactions are not surprising.\nAlso, it confirms that if there's just one parameter that should paid attention to, it's `splits`.\n\n### Main Effects\n\nNext, let's see the parameters' main effects, meaning the difference from the average predicted value (intercept) across the observed parameter values.  \nNote the varying y-axis scales --- they're kind of important here.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(patchwork)\np1 <- autoplot(rpglex, \"splits\")\np2 <- autoplot(rpglex, \"t_try\") \np3 <- autoplot(rpglex, \"max_interaction\")\np4 <- autoplot(rpglex, \"split_try\")\n\n(p1 + p2) / (p3 + p4)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/main-effects-1.png){width=672}\n:::\n:::\n\n\nSo, in short: `splits` wants to be large, `t_try` wants to be close to 1, `max_interaction` most likely also wants to be large and `split_try` is pulling a ¯\\\\\\_(ツ)_/¯ on us.  \nFair enough.\n\n### Interaction Effects\n\nWe can also take a look at the two largest interaction effects, `splits:split_try` and `splits:t_try`, but to be quite honest I'm not sure what to make of these plots except for how they illustrate in which direction MBO has taken the tuning process (large values for `splits` and `t_try`).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(rpglex, c(\"splits\", \"split_try\"))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/interaction-effects-1.png){width=672}\n:::\n\n```{.r .cell-code}\nautoplot(rpglex, c(\"splits\", \"t_try\"))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/interaction-effects-2.png){width=672}\n:::\n:::\n\n\nFinally, here's the final parameter configuration that \"won\", meaning these are the parameters I'll be using to fit an RPF to the `Bikeshare` data in a new version of the `glex` vignette:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntuned_rpf$tuning_result[, c(\"splits\", \"split_try\", \"t_try\", \"max_interaction\")]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   splits split_try     t_try max_interaction\n    <int>     <int>     <num>           <int>\n1:    100         6 0.9422986               6\n```\n\n\n:::\n\n```{.r .cell-code}\nc(rmse = sqrt(tuned_rpf$tuning_result$regr.mse))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    rmse \n35.79528 \n```\n\n\n:::\n:::\n\n\n...And thanks to `glex`, I guess I have a better intuition for these parameters now? \nIs that how it works?\nLet's say it does.\n\nI'm not entirely sure how much I want to trust these results or want to make generalizations based off of them (I don't), but the underlying principle seems quite useful to me.\nRPF is still a fairly young method and gaining intuition for its parameters like this seems neat.\n\n## Conclusion\n\nThe key takeaway for the `Bikeshare` tuning is that:\n\n- `splits` wants to be large.\n- `t_try` wants to be close to 1.\n- Setting `max_interaction` to 5 or greater is only going to make you wait for the result longer.\n- `split_try` is also a parameter that exists. Idunno maybejust wing it with that boi and be done with it.\n\nTurns out it wasn't particularly eye-opening to take a look at parameter interactions, but oh well.\nBetter to have decomposed and not needed it than to never decompose at all.\nOr something.\n\n[rpfarxiv]: https://arxiv.org/abs/2012.14563\n[rpfgithub]: https://github.com/PlantedML/randomPlantedForest\n[glexpaper]: https://proceedings.mlr.press/v206/hiabu23a.html\n[glexgithub]: https://github.com/PlantedML/glex\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}